{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Machine learning models, continued\n",
    "---\n",
    "### There are a countless number of machine learning 'models'; new ones are being invented by researchers all the time. \n",
    "\n",
    "### There is also an **ensemble methods** which just throws together a bunch of separate models to make a new model!\n",
    "\n",
    "\n",
    "---\n",
    "## **machine learning models, just in this chapter**:\n",
    "\n",
    "### linear regression\n",
    "### Ridge\n",
    "### Lasso\n",
    "### k nearest neigbors\n",
    "### logistic regression\n",
    "### decision trees\n",
    "### random forests\n",
    "### Naive Bayes\n",
    "### Ada Boost\n",
    "### XGBoost\n",
    "\n",
    "---\n",
    "## What is a model?\n",
    "\n",
    "### A **model** is simply some recipe or function that **takes in appropriate input** (often called features) and **returns appropriate output**\n",
    "\n",
    "---\n",
    "## Why do we need so many models?\n",
    "\n",
    "Because as data scientists we encounter different types of questions/tasks/problems, and some models are better suited to some problems than others. \n",
    "\n",
    "This is similar to how there are different types of mathematical functions, e.g \n",
    "- linear\n",
    "- quadratic\n",
    "- polynomial,\n",
    "- trigonometric\n",
    "- logarithmic\n",
    "\n",
    "that model different physical phenomena. For instance, in basic physics, the height of a falling object is modelled using a quadratic function of time t. \n",
    "The temperature might be modeled as a sinusoidal function of the month.\n",
    "---\n",
    "## Which is the best model to use?\n",
    "Trick question, as there is no right answer. Your job, if you become a data scientist, is to find a model (or collection of models) that gives answers that are **good enough** for your purposes!\n",
    "\n",
    "You should have a general sense of available models and their use cases, but pick a model (or create your own) is an art not a science! And you don't just pick one model and be done with it, you experiment with different models, combine different models, etc.\n",
    "\n",
    "That is what Kaggle Competitions and their leaderboards are used for - to improve your model, learn from others who are getting better results, etc.\n",
    "\n",
    "Remember, **we have given up hope of getting the output exactly right** - there is usually no (useful) exact formula for the data science problem at hand (e.g. if I give you the number of rooms, the crime rate, etc there is simply no way you can be expected to **exactly** compute the home price.) \n",
    "\n",
    "---\n",
    "## What makes a model good enough?\n",
    "Depends on what your problem is. If you are working on self-driving cars or disease detection, the threshold for *good enough* should be higher than if you are making an app that classifies if a picture is a cat or a dog!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ChatGPT constructively to learn \n",
    "\n",
    "## Ask ChatGPT to explain what each of these machine learning models is\n",
    "\n",
    "(Me to ChatGPT): Explain what the following machine learning models are. Perhaps make a table\n",
    "\n",
    "- linear regression\n",
    "- Ridge\n",
    "- Lasso\n",
    "- logistic regression\n",
    "- decision trees\n",
    "- random forests\n",
    "- Naive Bayes\n",
    "- Ada Boost\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT response (but read legitmate sources (our text, wikipedia) to be sure!)\n",
    "\n",
    "Sure, I'd be happy to explain these machine learning models! Here's a brief description of each one, along with some key characteristics and differences. I'll also provide a table summarizing the information.\n",
    "\n",
    "| Model           | Description                                                                                          | Key Characteristics                                                                                   |\n",
    "|-----------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| Linear Regression | Predicts a continuous target variable based on linear relationships between input features.       | Assumes a linear relationship, sensitive to outliers, uses least squares to minimize prediction error.|\n",
    "| Ridge           | A regularization technique applied to **linear regression**, adding a penalty term to the loss function to prevent overfitting.| Controls model complexity by adding a L2 regularization term, shrinks coefficient values towards zero. |\n",
    "| Lasso           | Similar to Ridge but uses L1 regularization, which can lead to some coefficients becoming exactly zero.| Used for feature selection, helps create sparse models, useful for reducing dimensionality.          |\n",
    "| Logistic Regression | Used for binary **classification**, estimates the probability of an instance belonging to a certain class.| Utilizes the logistic function to transform linear predictions into probabilities.                    |\n",
    "| Decision Trees  | Hierarchical structure that makes decisions by splitting data based on feature values.             | Easily interpretable, prone to overfitting, can handle both numerical and categorical data.          |\n",
    "| Random Forests  | Ensemble of decision trees, each trained on a random subset of data, and averages their predictions.| Reduces overfitting, improves generalization, can handle large datasets and high-dimensional data. |\n",
    "| Naive Bayes     | Probabilistic **classifier** based on Bayes' theorem, assumes features are independent given the class. | Works well with high-dimensional data, computationally efficient, \"naive\" assumption may limit accuracy.|\n",
    "| Ada Boost       | **Ensemble method** that sequentially combines weak learners to create a strong **classifier**.           | Focuses on instances with higher misclassification rates, assigns weights to data points.             |\n",
    "| XGBoost         | Extreme Gradient Boosting, an advanced boosting algorithm that optimizes the **ensemble** creation process.| Uses a more complex regularization approach, handles missing data, supports custom optimization objectives.|\n",
    "\n",
    "Remember that each of these models has its own strengths and weaknesses, and the choice of model depends on the specific problem you're trying to solve, the nature of your data, and the trade-offs you're willing to make between interpretability, accuracy, and computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, that's a lot to take in. Let organize it based on the two types of prediction problems we've seen\n",
    "---\n",
    "## Regression (predict a continous numerical value, like house price)\n",
    "- Linear Regression\n",
    "    - Ridge, Lasso are variants of Linear regression  ** regularization ** \n",
    "\n",
    "- k nearest neighbors\n",
    "- decision trees, random forests\n",
    "\n",
    "---\n",
    "\n",
    "## Classification (predict a discrete value or category, like cat or dog)\n",
    "- Logistic Regression (regression is a misnomer! I wish it was called logistic classification), applies to Binary **Classification** i.e. where there are two categories, like cat or dog\n",
    "- k nearest neighbors\n",
    "- decision trees, random forests\n",
    "---\n",
    "\n",
    "## Bagging methods (can apply to regression or classification)\n",
    "- Decision Trees, Random Forest are an ensemble (collection) of decision trees\n",
    "- Naive Bayes\n",
    "---\n",
    "\n",
    "## Boosting algorithms (can apply to regression or classification)\n",
    "- Idea is to modify weights that the model got wrong\n",
    "- AdaBoost, XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train split\n",
    "\n",
    "We split the data into two sets- train and test sets. The training set is like giving a practice exam to study for. The test is the actual test, which they do no see.\n",
    "\n",
    "\n",
    "## Overfitting\n",
    "Analogy:\n",
    "This is like a student who memorizes the answer to practice exams, and so does well on practice exams (or data has seen before) but does not do well on questions they have not seen before.\n",
    "\n",
    "## Regularization\n",
    "Analogy:\n",
    "Penalty for memorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
